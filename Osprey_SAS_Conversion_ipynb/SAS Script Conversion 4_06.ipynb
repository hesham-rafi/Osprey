{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import datetime\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#riskpf_path = '/group/axa_malaysia/data/adm_riskpf'\n",
    "#transv_pr_gr_psea_path = 'data/sas_402/transv_pr_gr_psea.parquet'\n",
    "#transv_polhistory_psea_path = 'data/sas_401/transv_polhistory_psea.parquet'\n",
    "#transv_pol_psea_path = 'data/sas_401/transv_pol_psea.parquet'\n",
    "#acc_yrm = 201707\n",
    "\n",
    "def format_date(strdate):\n",
    "    try: \n",
    "        return datetime.strptime(str(strdate),'%Y%m%d').strftime('%Y-%m-%d')\n",
    "    except: \n",
    "        return '2999-12-31'\n",
    "_format_date = udf(format_date,StringType())\n",
    "\n",
    "def pillar3(riskpf_path, transv_pr_gr_psea_path, transv_polhistory_psea_path, transv_pol_psea_path, acc_yrm, output_folder='data/sas_406/' ):\n",
    "    # /****************************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\tP3 TABLE: GWP & RISK EXPOSURE \t\t\t\t\t\t\t\t\t*\n",
    "    # *\t\t\t(KEY: chdrnum, rskno, tranno, zrenno, D_from, rskno) \t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # ****************************************************************************************/\n",
    "\n",
    "    # /****************************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t(1) SUMMARIZE PREMIUM TABLE BY rskno & tranno \t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # ****************************************************************************************/\n",
    "    # Remove 'future' rows - not in scope. The summary is by riskno and tranno only.\n",
    "    # The addition variables is added just for additional information - for example tranno will change with trantype (at least) but \n",
    "    # not the other way round\n",
    "\n",
    "    riskpf = spark.read.parquet(riskpf_path)\n",
    "    transv_pr_gr_psea = spark.read.parquet(transv_pr_gr_psea_path)\n",
    "    transv_polhistory_psea = spark.read.parquet(transv_polhistory_psea_path)\n",
    "    transv_pol_psea = spark.read.parquet(transv_pol_psea_path)\n",
    "\n",
    "    # Testing script - this one to be used for testing\n",
    "    prem = transv_pr_gr_psea\\\n",
    "    .filter(col('yrm') <= acc_yrm)\\\n",
    "    .groupBy('chdrnum','tranno','rskno','d_eff','d_exp','trantype','yrm').sum('gwp','cwp','gwc','cwc')\\\n",
    "    .withColumnRenamed('sum(gwp)','gwp').withColumnRenamed('sum(cwp)','cwp')\\\n",
    "    .withColumnRenamed('sum(gwc)','gwc').withColumnRenamed('sum(cwc)','cwc')\n",
    "\n",
    "    # Testing script - this one to be used for testing\n",
    "    polhist = transv_polhistory_psea[['chdrnum','tranno','zrenno','d_cancel']].orderBy('chdrnum','tranno')\n",
    "    polhist.cache()\n",
    "\n",
    "    prem2 = prem.join(polhist, on=['chdrnum','tranno'], how='left')\\\n",
    "    .withColumn('gwp2',col('gwp')-col('cwp'))\\\n",
    "    .withColumn('gwc2',col('gwc')-col('cwc'))\\\n",
    "    .drop('gwp','gwc').withColumnRenamed('gwp2','gwp').withColumnRenamed('gwc2','gwc')\\\n",
    "    .groupBy('chdrnum','rskno','zrenno','d_eff','d_exp').sum('gwp','gwc')\\\n",
    "    .withColumnRenamed('sum(gwp)','gwp').withColumnRenamed('sum(gwc)','gwc')\\\n",
    "    .filter(abs(col('gwp'))>0.01)\n",
    "\n",
    "    # /********************************************************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # *\t(3) IN THE NEWLY MERGED TABLE, INDEX THE DIFFERENT PERIODS (chdrnum * rskno * zrenno * d_from * d_to) \t\t\t*\n",
    "    # *\t\t\t\t\t\t\tAND CALCULATE THE DAILY RATES FOR EACH PERIOD \t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # ********************************************************************************************************************/\n",
    "    prem3 = prem2\\\n",
    "    .withColumn('daily_rate_p',(col('gwp')/(datediff(col('d_exp'),col('d_eff'))+1)))\\\n",
    "    .withColumn('daily_rate_c',(col('gwc')/(datediff(col('d_exp'),col('d_eff'))+1)))\\\n",
    "    .withColumn('period', row_number().over(Window.partitionBy('chdrnum','rskno','zrenno').orderBy('chdrnum','rskno','zrenno','d_eff','d_exp')))\\\n",
    "    .drop('gwp','gwc')\n",
    "    prem3.cache()\n",
    "\n",
    "    # /********************************************************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # *\t\t(4) TO SPLIT THE DIFFERENT PERIODS THE RIGHT AMOUNT OF TIMES, GET LIST OF ALL POSSIBLE \t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t\t\tEFFECTIVE DATES WITHIN EACH PERIOD \t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t*\n",
    "    # ********************************************************************************************************************/\n",
    "    listdates = prem3[['chdrnum','rskno','zrenno','d_eff','d_exp']]\\\n",
    "    .withColumn('d_temp',explode(array(col('d_eff'),col('d_exp')))).drop('d_eff','d_exp')\\\n",
    "    .orderBy('chdrnum','rskno','zrenno','d_temp')\\\n",
    "    .dropDuplicates(['chdrnum','rskno','zrenno','d_temp'])\n",
    "    listdates.cache()\n",
    "\n",
    "    # /*********************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t *\n",
    "    # *\t(5) GET THE NUMBER OF PERIODS FOR EACH POLICY x RISK PERIOD (zrenno). \t\t *\n",
    "    # *\t \t\tWHEN THERE IS ONLY 1, WE WON'T SPLIT THE DATES. \t\t\t\t\t *\n",
    "    # *\t\t\t\t   MAP THE COUNT TO THE LISTDATES TABLE \t \t\t\t\t\t *\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t *\n",
    "    # *********************************************************************************/\n",
    "    nbperiod = prem3[['chdrnum','rskno','zrenno','period']]\\\n",
    "    .orderBy('chdrnum','rskno','zrenno')\\\n",
    "    .withColumn('periodcount',col('period'))\\\n",
    "    .withColumn('last',row_number().over(Window.partitionBy('chdrnum','rskno','zrenno').orderBy(desc('chdrnum'),\n",
    "                                                                                                desc('rskno'),\n",
    "                                                                                                desc('zrenno'),\n",
    "                                                                                                desc('periodcount'))))\\\n",
    "    .filter(col('last')==1).drop('last')\n",
    "\n",
    "    nbperiod.cache()\n",
    "\n",
    "    nbperiod_inter = nbperiod.select('chdrnum','rskno','zrenno','periodcount')\n",
    "    listdates2 = listdates.join(nbperiod_inter,on=['chdrnum','rskno','zrenno'],how='left')\n",
    "\n",
    "    # /*********************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t *\n",
    "    # *\t(6) IN THE PREMIUM SUMMARY TABLE, MAP THE DIFFERENT EFFECTIVE DATES ON \t\t *\n",
    "    # *\t\tTHE PERIODS WHERE THE TOTAL COUNT OF PERIODS PER zrenno IS MORE THAN 1 \t *\n",
    "    # * \t\t\t\t(RQ: THIS WILL NATURALLY DUPLICATE THE RECORDS) \t\t\t \t *\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t *\n",
    "    # *********************************************************************************/\n",
    "    cond = [prem3['chdrnum']==listdates2['chdrnum'],\n",
    "            prem3['rskno']==listdates2['rskno'],\n",
    "            prem3['zrenno']==listdates2['zrenno'], \n",
    "            (((prem3['d_eff'] <= listdates2['d_temp']) & (listdates2['d_temp'] <=prem3['d_exp']) ) & (listdates2['periodcount'] > 1))]\n",
    "    prem4 = prem3.join(listdates2,cond,how='left').select([prem3[xx] for xx in prem3.columns]+[listdates2['d_temp']])\\\n",
    "    .orderBy('chdrnum','rskno','zrenno','period','d_temp')\n",
    "    prem4.cache()\n",
    "\n",
    "    # /*********************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t *\n",
    "    # *\t\t(7) SPLIT THE RECORDS BY ALL THE DIFFERENT EFFECTIVE DATES \t\t\t\t *\n",
    "    # *\t\t\t\t\t\tWHICH WERE MAPPED IN STEP 6 \t\t \t\t\t\t\t *\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t *\n",
    "    # *********************************************************************************/\n",
    "    prem5 = prem4\\\n",
    "    .withColumn('d_temp_prev', lag(col('d_temp')).over(Window.partitionBy('chdrnum','rskno','zrenno','period').orderBy('d_temp')))\\\n",
    "    .withColumn('d_count', count(col('chdrnum')).over(Window.partitionBy('chdrnum','rskno','zrenno','period')))\\\n",
    "    .withColumn('d_from', when(col('d_temp')==date_add(col('d_temp_prev'),1), col('d_temp')).otherwise(date_add(col('d_temp_prev'),1)))\\\n",
    "    .withColumn('d_to',  when(col('d_temp')==date_add(col('d_temp_prev'),1), col('d_temp')).otherwise(date_add(col('d_temp'),-1)))\\\n",
    "    .withColumn('first', struct(col('d_from').alias('from'), col('d_to').alias('to')))\\\n",
    "    .withColumn('second', struct(col('d_temp').alias('from'), col('d_temp').alias('to')))\\\n",
    "    .withColumn('final', explode(array(col('first'), col('second'))))\\\n",
    "    .withColumn('d_from', when(col('d_count')==1, col('d_eff')).otherwise(col('final')['from']))\\\n",
    "    .withColumn('d_to', when(col('d_count')==1, col('d_exp')).otherwise(col('final')['to']))\\\n",
    "    .drop(\"first\",\"second\",\"final\")\\\n",
    "    .filter(col('d_from').isNotNull())\\\n",
    "    .drop_duplicates(['chdrnum','rskno','zrenno','d_from','d_to','period'])\\\n",
    "    .groupBy('chdrnum','rskno','zrenno','d_from','d_to').sum('daily_rate_p','daily_rate_c')\\\n",
    "    .withColumnRenamed('sum(daily_rate_p)','daily_rate_p').withColumnRenamed('sum(daily_rate_c)','daily_rate_c')\n",
    "    prem5.cache()\n",
    "\n",
    "    # /************************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \t*\n",
    "    # *  (8) FOR EACH RECORD IN OUR NEW PREM5, WE'D LIKE A tranno TO BE ABLE TO MERGE   \t*\n",
    "    # *\t\t\t\t\t\t\tWITH ANY RISK TABLE LATER ON.\t\t\t\t\t\t\t*\n",
    "    # *\t\tWE FIND THE CORRESPONDING tranno IN RISKPF BASED ON THE EFFECTIVE DATES\t  \t*\n",
    "    # *  (d_from IN PREM5 MATCHING dteeff IN RISKPF (OR THE CLOSEST MATCH IN THE zrenno).\t*\n",
    "    # *\t\t\tFOR THE ONES WHERE THERE IS NO POSSIBLE MATCHING tranno,\t\t\t\t*\n",
    "    # *\t\t\t\t\t  WE OUTPUT THE RECORD IN ERROR TABLE.\t\t\t\t\t\t\t*\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \t*\n",
    "    # ************************************************************************************/\n",
    "    riskpf2 = riskpf[['chdrno','tranno','rskno','dteatt','dteeff','dteter','datime','rsktyp','recformat']]\\\n",
    "    .orderBy(['chdrno','tranno','rskno','datime'],ascending=[1,1,1,0]).drop('datime')\\\n",
    "    .dropDuplicates(['chdrno','tranno','rskno'])\n",
    "\n",
    "    polhist_risk = riskpf2.join(polhist.withColumnRenamed('chdrnum','chdrno'),on=['chdrno','tranno'],how='inner')\\\n",
    "    .withColumn('d_starteff', to_date(_format_date(col('dteeff'))))\\\n",
    "    .select('chdrno','rskno','zrenno','tranno','d_starteff','dteter','rsktyp','recformat')\\\n",
    "    .orderBy(['chdrno','rskno','zrenno','d_starteff','dteter','tranno'],ascending=[1,1,1,0,0,0])\n",
    "    polhist_risk.cache()\n",
    "\n",
    "    # /**************************************************************************************************************\n",
    "    # *  We Map the closest transaction of RISKPF to our PREM TABLE. If not a perfect match (d_starteff NE d_from)  *\n",
    "    # *\t\t\t\t\t\t\t then we move on to the previous d_starteff.\t\t\t\t\t\t\t\t\t  *\n",
    "    # * \t\tWe also map the lob and date of cancellation (useful when we calculate exposure) from Policy Header   *\n",
    "    # **************************************************************************************************************/\n",
    "\n",
    "    # Read in pol_psea\n",
    "    pol_psea = transv_pol_psea[['chdrnum','d_cancel','chdrstcdc']]\n",
    "    # Have to define udf to handle dynamic substring function\n",
    "    _substring_udf = udf(lambda x: x[0:len(x)-3])\n",
    "    polhist_risk2 = polhist_risk.withColumnRenamed('chdrno','chdrnum').withColumn('rsktabl', _substring_udf(col('recformat'))).drop('recformat')\n",
    "\n",
    "\n",
    "    big = prem5.join(polhist_risk2, on=(['chdrnum','rskno','zrenno']),how='inner')\\\n",
    "    .join(pol_psea, on=(['chdrnum']), how ='left')\n",
    "    big.cache()\n",
    "\n",
    "    prem6 = big.filter(col('d_from') >= col('d_starteff'))\\\n",
    "    .withColumn('indicator',row_number().over(\n",
    "            Window.partitionBy('chdrnum','rskno','zrenno','d_from','d_to').orderBy(desc('d_starteff'),desc('dteter'),desc('tranno'))))\\\n",
    "    .filter(col('indicator') == 1)\\\n",
    "    .select('chdrnum','zrenno','rskno','d_from','d_to','d_cancel','tranno','chdrstcdc','daily_rate_p','daily_rate_c','rsktyp','rsktabl')\n",
    "\n",
    "    error_inter = big.withColumn('condition',when(col('d_from') >= col('d_starteff'), lit(1)).otherwise(lit(0)))\\\n",
    "    .groupBy('chdrnum','rskno','zrenno').sum('condition').filter(col('sum(condition)')==0)\n",
    "\n",
    "    error = big.join(error_inter,on=(['chdrnum','rskno','zrenno']),how='inner')\\\n",
    "    .select('chdrnum','zrenno','rskno','d_from','d_to','d_cancel','chdrstcdc','daily_rate_p','daily_rate_c','rsktyp','rsktabl')\n",
    "\n",
    "    #/**********************************************************************************\n",
    "    #*\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  *\n",
    "    #*\t  (9) FOR SEVERAL RECORDS, WE MAY NOT HAVE FOUND ANY POSSIBLE CANDIDATE \t  *\n",
    "    #*\t\t\t\t\t\tFOR A tranno WITHIN THE SAME POI. \t\t\t\t\t\t  *\n",
    "    #*\t\t\tWE THEN MAP THE INFO FROM THE FIRST RECORD OF POLHIST_RISK.\t\t\t  *\n",
    "    #*\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  *\n",
    "    #**********************************************************************************/\n",
    "\n",
    "    cond = [error['chdrnum']==polhist_risk['chdrno'],\n",
    "            error['rskno'] == polhist_risk['rskno']]\n",
    "    error2 = error.join(polhist_risk, cond, how='left')\\\n",
    "    .select([error[xx] for xx in error.columns] + [polhist_risk['tranno'],polhist_risk['d_starteff']])\\\n",
    "    .orderBy('chdrnum','rskno','zrenno','d_from','d_to',desc('d_starteff'))\\\n",
    "    .dropDuplicates(['chdrnum','rskno','zrenno','d_from','d_to']).drop('d_starteff')\n",
    "\n",
    "    # /**********************************************************************************\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  *\n",
    "    # * (10) FINAL TABLE IS THE CONCATENATION OF THE PREVIOUS AND THE CORRECTED ERRORS. *\n",
    "    # *\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  *\n",
    "    # **********************************************************************************/\n",
    "\n",
    "    transv_p3_psea = prem6.union(error2.select(prem6.columns)).orderBy('chdrnum','rskno','zrenno','d_from')\n",
    "    transv_p3_psea.write.parquet('{}transv_p3_psea.parquet'.format(output_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transv_p3_psea_path = 'data/sas_406/transv_p3_psea.parquet'\n",
    "#acc_yr = 2017\n",
    "#acc_mth = 7\n",
    "#period = 5\n",
    "\n",
    "def loop_yrm(transv_p3_psea_path, acc_yr, acc_mth, period, output_folder='data/sas_406/'):\n",
    "    \n",
    "    transv_p3_psea = spark.read.parquet(transv_p3_psea_path)\n",
    "    transv_p3_psea.cache()\n",
    "\n",
    "    # /* LOOP ON DIFFERENT YRMs */\n",
    "    earn_start = datetime.datetime.strptime('{}-01-01'.format(acc_yr - (period + 2)), '%Y-%m-%d')\n",
    "    earn_end = datetime.date(acc_yr, acc_mth, calendar.monthrange(acc_yr,acc_mth)[1])\n",
    "\n",
    "    # Compute the number of years required to run all months\n",
    "    n = relativedelta(earn_end, earn_start)\n",
    "    n = n.years*12 + n.months + 1\n",
    "\n",
    "    list_range = []\n",
    "    for i in range(np.abs(n)):\n",
    "        u_bound = earn_end - relativedelta(months=(i)) # Subtracting those months might push to different year\n",
    "        l_bound = datetime.date(u_bound.year,u_bound.month, 1) # Same as u_bound, just beginning of month\n",
    "        year_length = (datetime.date(u_bound.year,12,31) - datetime.date(u_bound.year,1,1)).days + 1\n",
    "        yrm = int(u_bound.strftime('%Y%m'))\n",
    "        list_range.append(\n",
    "        {'u_bound':u_bound,\n",
    "          'l_bound':l_bound,\n",
    "          'year_length':year_length,\n",
    "          'yrm':yrm})\n",
    "    list_range_bc = sc.broadcast(list_range)\n",
    "\n",
    "\n",
    "    def get_valid_range(d_from, d_to):\n",
    "        return [d for d in list_range_bc.value if ((d_from <= d[\"u_bound\"]) and (d_to >= d[\"l_bound\"]))]\n",
    "    _get_valid_range = udf(get_valid_range, ArrayType(\n",
    "            StructType(\n",
    "                [StructField('u_bound',DateType()),\n",
    "                StructField('l_bound',DateType()),\n",
    "                StructField('year_length',ShortType()),\n",
    "                StructField('yrm',IntegerType())])))\n",
    "\n",
    "    p3_psea_monthly = transv_p3_psea.withColumn('temp', _get_valid_range(col('d_from'),col('d_to')))\\\n",
    "    .withColumn('temp',explode('temp'))\\\n",
    "    .withColumn('u_bound',col('temp')['u_bound'])\\\n",
    "    .withColumn('l_bound',col('temp')['l_bound'])\\\n",
    "    .withColumn('year_length',col('temp')['year_length'])\\\n",
    "    .withColumn('yrm',col('temp')['yrm']).drop('temp')\\\n",
    "    .withColumn('gep',((datediff(least(col('d_to'),col('u_bound')), greatest(col('d_from'),col('l_bound')) )+1) * col('daily_rate_p')))\\\n",
    "    .withColumn('gec',((datediff(least(col('d_to'),col('u_bound')), greatest(col('d_from'),col('l_bound')) )+1) * col('daily_rate_c')))\\\n",
    "    .withColumn('exp', when(col('d_cancel') < col('l_bound'), lit(0))\\\n",
    "               .when(col('d_cancel') > col('u_bound'), datediff(col('u_bound'),greatest(col('d_from'),col('l_bound')))/col('year_length'))\\\n",
    "               .otherwise(datediff(least(col('d_to'),col('u_bound')), date_add(greatest(col('d_from'),col('l_bound')),1))/col('year_length')))\\\n",
    "    .withColumn('rif', when(col('d_cancel') < col('l_bound'), lit(0))\\\n",
    "                .when( col('d_cancel') > col('u_bound'), \n",
    "                     when((col('d_from') <= col('u_bound')) & (col('d_to') > col('u_bound')), lit(1)).otherwise(lit(0)))\\\n",
    "                .otherwise(lit(0))\n",
    "               )\\\n",
    "    .groupBy('chdrnum','rskno','zrenno','tranno','rsktyp','rsktabl','chdrstcdc','yrm')\\\n",
    "    .sum('gep','gec','exp','rif')\\\n",
    "    .withColumnRenamed('sum(gep)','gep').withColumnRenamed('sum(gec)','gec')\\\n",
    "    .withColumnRenamed('sum(exp)','exp').withColumnRenamed('sum(rif)','rif')\n",
    "\n",
    "    p3_psea_monthly.write.parquet('{}transv_p3_psea_monthly.parquet'.format(output_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transv_cl_quali_psea_path = 'data/sas_403/transv_cl_quali_psea.parquet'\n",
    "#transv_cl_quanti_psea_path = 'data/sas_403/transv_cl_quanti_psea.parquet'\n",
    "#transv_polhistory_psea_path = 'data/sas_401/transv_polhistory_psea.parquet'\n",
    "#transv_p3_psea_path = 'data/sas_406/transv_p3_psea.parquet'\n",
    "#transv_p3_psea_monthly_path = 'data/sas_406/transv_p3_psea_monthly.parquet'\n",
    "#adm_mapping_path = 'ADM Mapping.xlsm'\n",
    "#acc_yr = 2017\n",
    "#acc_mth = 7\n",
    "#period = 5\n",
    "#delay = 2\n",
    "\n",
    "def pillar3_psea_12rm(transv_cl_quali_psea_path, \n",
    "                      transv_cl_quanti_psea_path, \n",
    "                      transv_polhistory_psea_path,\n",
    "                      transv_p3_psea_path, \n",
    "                      transv_p3_psea_monthly_path,\n",
    "                      adm_mapping_path, \n",
    "                      acc_yr, acc_mth, period, delay, \n",
    "                      output_folder='data/sas_406/'):\n",
    "    \n",
    "    transv_cl_quali_psea = spark.read.parquet(transv_cl_quali_psea_path)\n",
    "    transv_cl_quanti_psea = spark.read.parquet(transv_cl_quanti_psea_path)\n",
    "    transv_polhistory_psea = spark.read.parquet(transv_polhistory_psea_path)\n",
    "    transv_p3_psea = spark.read.parquet(transv_p3_psea_path)\n",
    "    transv_p3_psea_monthly = spark.read.parquet(transv_p3_psea_monthly_path)\n",
    "    act_claim_largeclaim = spark.createDataFrame(pd.read_excel(adm_mapping_path,sheetname = '112'))\n",
    "\n",
    "    # Prepare claims\n",
    "    claim_quali = transv_cl_quali_psea[['claim','chdrnum','rskno','d_occ','chdrstcdc','natcat']]\\\n",
    "    .orderBy('claim').dropDuplicates(['claim'])\\\n",
    "    .withColumn('loss_yrm',(year(col('d_occ')) * 100 + month(col('d_occ'))))\n",
    "\n",
    "    # Get the mapping of trannos from P3\n",
    "    trannomap = transv_p3_psea[['chdrnum','rskno','d_from','d_to','zrenno','tranno','rsktabl']]\n",
    "\n",
    "    cond = [(claim_quali['chdrnum'] == trannomap['chdrnum']) &\n",
    "            (claim_quali['rskno'] == trannomap['rskno']) &\n",
    "            ((claim_quali['d_occ'] >= trannomap['d_from']) &\n",
    "            (claim_quali['d_occ'] <= trannomap['d_to']))]\n",
    "\n",
    "    claimquali2 = claim_quali.join(trannomap,cond,how='left')\\\n",
    "    .select([claim_quali[c] for c in claim_quali.columns] + \n",
    "            [trannomap['zrenno'], trannomap['tranno'], trannomap['d_from'], trannomap['rsktabl']])\\\n",
    "    .orderBy('claim','zrenno','d_from')\\\n",
    "    .dropDuplicates(['claim']).drop('d_from','d_to')\n",
    "\n",
    "    claimquanti = transv_cl_quanti_psea[['claim','yrm','tranno','d_tran','clstat','gpay','gmov']]\\\n",
    "    .groupBy('claim','tranno','d_tran','yrm','clstat').sum('gpay','gmov')\\\n",
    "    .withColumnRenamed('sum(gpay)','gpay').withColumnRenamed('sum(gmov)','gmov').withColumnRenamed('tranno','tranno_cl')\n",
    "\n",
    "    claim = claimquali2.join(claimquanti,on='claim',how='inner')\n",
    "\n",
    "    # Start of loops\n",
    "    p3_start = datetime.datetime.strptime('{}-01-01'.format(acc_yr - (period + 2)), '%Y-%m-%d')\n",
    "    p3_end = datetime.date(acc_yr, acc_mth, calendar.monthrange(acc_yr,acc_mth)[1])\n",
    "\n",
    "    # Computer the number of steps required to run all months\n",
    "    n = relativedelta(p3_end, p3_start)\n",
    "    n = n.years*12 + n.months + 1\n",
    "\n",
    "    list_range = []\n",
    "    for i in range(np.abs(n)):\n",
    "        vision_yrm = int((p3_end - relativedelta(months=(i))).strftime('%Y%m'))\n",
    "        l_bound_yrm = int((p3_end - relativedelta(months=(i)) - relativedelta(months=(11+delay))).strftime('%Y%m'))\n",
    "        u_bound_yrm = int((p3_end - relativedelta(months=(i)) - relativedelta(months=(delay))).strftime('%Y%m'))\n",
    "        list_range.append(\n",
    "        {'vision_yrm':vision_yrm,\n",
    "          'l_bound_yrm':l_bound_yrm,\n",
    "          'u_bound_yrm':u_bound_yrm})\n",
    "    list_range_bc = sc.broadcast(list_range)\n",
    "\n",
    "\n",
    "    def get_valid_range_premium(yrm):\n",
    "        return [d for d in list_range_bc.value if ((yrm >= d[\"l_bound_yrm\"]) and (yrm <= d[\"u_bound_yrm\"]))]\n",
    "    def get_valid_range_claim(loss_yrm, yrm):\n",
    "        return [d for d in list_range_bc.value if \n",
    "                ((loss_yrm >= d[\"l_bound_yrm\"]) and (loss_yrm <= d[\"u_bound_yrm\"]) and (yrm <= d[\"vision_yrm\"]))]\n",
    "\n",
    "    _get_valid_range_premium = udf(get_valid_range_premium, ArrayType(\n",
    "            StructType(\n",
    "                [StructField('vision_yrm',IntegerType()),\n",
    "                StructField('l_bound_yrm',IntegerType()),\n",
    "                StructField('u_bound_yrm',IntegerType())])))\n",
    "    _get_valid_range_claim = udf(get_valid_range_claim, ArrayType(\n",
    "            StructType(\n",
    "                [StructField('vision_yrm',IntegerType()),\n",
    "                StructField('l_bound_yrm',IntegerType()),\n",
    "                StructField('u_bound_yrm',IntegerType())])))\n",
    "\n",
    "    # 12 RM premium from p3_monthly\n",
    "    premiums = transv_p3_psea_monthly.withColumn('temp', _get_valid_range_premium(col('yrm')))\\\n",
    "    .withColumn('temp',explode('temp'))\\\n",
    "    .withColumn('yrm',col('temp')['vision_yrm'])\\\n",
    "    .withColumn('l_bound_yrm',col('temp')['l_bound_yrm'])\\\n",
    "    .withColumn('u_bound_yrm',col('temp')['u_bound_yrm'])\\\n",
    "    .drop('temp')\\\n",
    "    .groupBy('chdrnum','rskno','zrenno','tranno','rsktabl','yrm').sum('gep','gec','exp','rif')\\\n",
    "    .withColumnRenamed('sum(gep)','gep').withColumnRenamed('sum(gec)','gec')\\\n",
    "    .withColumnRenamed('sum(exp)','exp').withColumnRenamed('sum(rif)','rif')\n",
    "\n",
    "    # 12 RM claims - split between attritional/catnat/large + exclude claims closed @ nil from the count\n",
    "    tempcl = claim.withColumn('temp', _get_valid_range_claim(col('loss_yrm'),col('yrm')))\\\n",
    "    .withColumn('temp',explode('temp'))\\\n",
    "    .withColumn('yrm',col('temp')['vision_yrm'])\\\n",
    "    .withColumn('l_bound_yrm',col('temp')['l_bound_yrm'])\\\n",
    "    .withColumn('u_bound_yrm',col('temp')['u_bound_yrm'])\\\n",
    "    .drop('temp')\\\n",
    "    .withColumn('lastclaim',row_number().over(Window.partitionBy('yrm','claim').orderBy(desc('tranno_cl'),desc('d_tran'))))\\\n",
    "    .withColumn('gpay_ult',sum('gpay').over(Window.partitionBy('yrm','claim','tranno','d_tran').orderBy('tranno_cl','d_tran')))\\\n",
    "    .withColumn('gmov_ult',sum('gmov').over(Window.partitionBy('yrm','claim','tranno','d_tran').orderBy('tranno_cl','d_tran')))\\\n",
    "    .filter(col('lastclaim')==1)\n",
    "\n",
    "    tempcl = tempcl.join(act_claim_largeclaim, on='chdrstcdc',how='left')\\\n",
    "    .withColumn('threshold_breach', \n",
    "                when(col('threshold').isNotNull() & ((col('gpay_ult') + col('gmov_ult')) > col('threshold')),\n",
    "                     True).otherwise(False))\\\n",
    "    .withColumn('ginc_nc', when(col('natcat')==1, col('gpay_ult')+col('gmov_ult')).otherwise(lit(0)) )\\\n",
    "    .withColumn('nbclaim_nc', when(col('natcat')==1, lit(1)).otherwise(lit(0)) )\\\n",
    "    .withColumn('ginc_l', \n",
    "                when((col('natcat') != 1) & (col('threshold_breach')==True), col('gpay_ult') + col('gmov_ult')).otherwise(lit(0)) )\\\n",
    "    .withColumn('nbclaim_l', when((col('natcat') != 1) & (col('threshold_breach')==True), lit(1)).otherwise(lit(0)) )\\\n",
    "    .withColumn('ginc_a',\n",
    "                when((col('natcat') != 1) & (col('threshold_breach')==False), col('gpay_ult') + col('gmov_ult')).otherwise(lit(0)) )\\\n",
    "    .withColumn('nbclaim_a', when((col('natcat') != 1) & (col('threshold_breach')==False), lit(1)).otherwise(lit(0)) )\\\n",
    "    .withColumn('closed_notpaid', when(((col('clstat')==lit('2')) & (abs(col('gpay_ult') + col('gmov_ult')) < 0.01)), True).otherwise(False))\\\n",
    "    .withColumn('nbclaim_nc', when((col('closed_notpaid')==True), lit(0)).otherwise(col('nbclaim_nc')))\\\n",
    "    .withColumn('nbclaim_l', when((col('closed_notpaid')==True), lit(0)).otherwise(col('nbclaim_l')))\\\n",
    "    .withColumn('nbclaim_a', when((col('closed_notpaid')==True), lit(0)).otherwise(col('nbclaim_a')))\\\n",
    "    .select('chdrnum','rskno','zrenno','tranno','yrm','rsktabl','ginc_a','ginc_l','ginc_nc','nbclaim_a','nbclaim_l','nbclaim_nc')\n",
    "\n",
    "    claims = tempcl.groupBy('chdrnum','rskno','zrenno','tranno','yrm','rsktabl')\\\n",
    "    .sum('ginc_a','ginc_l','ginc_nc','nbclaim_a','nbclaim_l','nbclaim_nc')\\\n",
    "    .withColumnRenamed('sum(ginc_a)','ginc_a').withColumnRenamed('sum(ginc_l)','ginc_l').withColumnRenamed('sum(ginc_nc)','ginc_nc')\\\n",
    "    .withColumnRenamed('sum(nbclaim_a)','nbclaim_a').withColumnRenamed('sum(nbclaim_l)','nbclaim_l').withColumnRenamed('sum(nbclaim_nc)','nbclaim_nc')\n",
    "\n",
    "\n",
    "    # Setting up the premiums and claims dataset for merging\n",
    "    cmissing = set(premiums.columns).difference(set(claims.columns))\n",
    "    pmissing = set(claims.columns).difference(set(premiums.columns))\n",
    "\n",
    "    for i in iter(pmissing):\n",
    "        premiums = premiums.withColumn(i,lit(None))\n",
    "\n",
    "    for i in iter(cmissing):\n",
    "        claims = claims.withColumn(i,lit(None))\n",
    "\n",
    "    p3_12rm = premiums.union(claims.select(premiums.columns))\n",
    "\n",
    "    p3_12rm = p3_12rm.fillna(0)\n",
    "    p3_psea_12rm = p3_12rm.groupBy('chdrnum','rskno','zrenno','tranno','yrm','rsktabl')\\\n",
    "    .sum('gep','gec','exp','rif','ginc_a','ginc_l','ginc_nc','nbclaim_a','nbclaim_l','nbclaim_nc')\\\n",
    "    .withColumnRenamed('sum(gep)','gep').withColumnRenamed('sum(gec)','gec').withColumnRenamed('sum(exp)','exp')\\\n",
    "    .withColumnRenamed('sum(rif)','rif').withColumnRenamed('sum(ginc_a)','ginc_a').withColumnRenamed('sum(ginc_l)','ginc_l')\\\n",
    "    .withColumnRenamed('sum(ginc_nc)','ginc_nc').withColumnRenamed('sum(nbclaim_a)','nbclaim_a')\\\n",
    "    .withColumnRenamed('sum(nbclaim_l)','nbclaim_l').withColumnRenamed('sum(nbclaim_nc)','nbclaim_nc')\n",
    "\n",
    "    transv_p3_psea_12rm = p3_psea_12rm.join(transv_polhistory_psea[['chdrnum','tranno','cnttype','chdrstcdc','agentid']], \n",
    "                                      on=['chdrnum','tranno'], how = 'left')\n",
    "    transv_p3_psea_12rm.write.parquet('{}transv_p3_psea_12rm.parquet'.format(output_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "riskpf_path = '/group/axa_malaysia/data/adm_riskpf'\n",
    "transv_pr_gr_psea_path = 'data/sas_402/transv_pr_gr_psea.parquet'\n",
    "transv_polhistory_psea_path = 'data/sas_401/transv_polhistory_psea.parquet'\n",
    "transv_pol_psea_path = 'data/sas_401/transv_pol_psea.parquet'\n",
    "acc_yrm = 201707\n",
    "pillar3(riskpf_path, transv_pr_gr_psea_path, transv_polhistory_psea_path, transv_pol_psea_path, acc_yrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transv_p3_psea_path = 'data/sas_406/transv_p3_psea.parquet'\n",
    "acc_yr = 2017\n",
    "acc_mth = 7\n",
    "period = 5\n",
    "loop_yrm(transv_p3_psea_path, acc_yr, acc_mth, period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transv_cl_quali_psea_path = 'data/sas_403/transv_cl_quali_psea.parquet'\n",
    "transv_cl_quanti_psea_path = 'data/sas_403/transv_cl_quanti_psea.parquet'\n",
    "transv_polhistory_psea_path = 'data/sas_401/transv_polhistory_psea.parquet'\n",
    "transv_p3_psea_path = 'data/sas_406/transv_p3_psea.parquet'\n",
    "transv_p3_psea_monthly_path = 'data/sas_406/transv_p3_psea_monthly.parquet'\n",
    "adm_mapping_path = 'ADM Mapping.xlsm'\n",
    "acc_yr = 2017\n",
    "acc_mth = 7\n",
    "period = 5\n",
    "delay = 2\n",
    "pillar3_psea_12rm(transv_cl_quali_psea_path, transv_cl_quanti_psea_path, transv_polhistory_psea_path,\n",
    "                 transv_p3_psea_path, transv_p3_psea_monthly_path, adm_mapping_path,\n",
    "                 acc_yr, acc_mth, period, delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15742409"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('/user/cchin/data/sas_406/transv_p3_psea.parquet').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118053853"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('/user/cchin/data/sas_406/transv_p3_psea_monthly.parquet').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195550693"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('/user/cchin/data/sas_406/transv_p3_psea_12rm.parquet').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.0.0 - YARN [anaconda3-4.1.1]",
   "language": "",
   "name": "pyspark2_yarn_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
